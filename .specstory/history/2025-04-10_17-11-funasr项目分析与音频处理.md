<!-- Generated by SpecStory -->

# FunASR项目分析与音频处理 (2025-04-11 01:11:08)

_**User**_

分析整个项目 @xiaozhi-server  我需要把我麦克风收录的声音，生成的中间文件保存下来，不要删除。我需要分析下为什么FunASR的识别是错误的，我用同样的中文音频及原模型调用方式就可以，但是项目中就会被识别错误。

以下是FunASR模型流式调用的参考代码：
from funasr import AutoModel

chunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms
encoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention
decoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention

model = AutoModel(model="paraformer-zh-streaming")

import soundfile
import os

wav_file = os.path.join(model.model_path, "example/asr_example.wav")
speech, sample_rate = soundfile.read(wav_file)
chunk_stride = chunk_size[1] * 960 # 600ms

cache = {}
total_chunk_num = int(len((speech)-1)/chunk_stride+1)
for i in range(total_chunk_num):
    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]
    is_final = i == total_chunk_num - 1
    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)
    print(res)

---

_**Assistant**_



---

好的，我们来分析一下这个问题。

首先，为了能够分析 FunASR 识别错误的具体原因，我们需要保留中间生成的音频文件。我将搜索项目中处理 FunASR 语音识别以及可能删除中间文件的代码。



---

<details>
            <summary>Searched codebase "funasr audio file delete" • **25** results</summary>
        
| File | Lines |
|------|-------|
| `main/xiaozhi-server/core/providers/asr/fun_local.py` | L33-74 |
| `main/xiaozhi-server/core/providers/asr/fun_local.py` | L74-110 |
| `main/xiaozhi-server/core/providers/asr/sherpa_onnx_local.py` | L131-164 |
| `main/xiaozhi-server/core/providers/asr/tencent.py` | L1-55 |
| `main/xiaozhi-server/core/providers/asr/doubao.py` | L83-122 |
| `main/xiaozhi-server/core/providers/asr/sherpa_onnx_local.py` | L84-131 |
| `main/xiaozhi-server/core/providers/asr/sherpa_onnx_local.py` | L35-84 |
| `main/xiaozhi-server/config.yaml` | L198-229 |
| `main/xiaozhi-server/core/providers/tts/base.py` | L1-44 |
| `main/xiaozhi-server/core/providers/asr/doubao.py` | L214-253 |
| `main/xiaozhi-server/core/providers/asr/fun_local.py` | L1-33 |
| `main/xiaozhi-server/core/providers/tts/fishspeech.py` | L64-72 |
| `main/xiaozhi-server/core/connection.py` | L688-723 |
| `main/xiaozhi-server/core/providers/tts/fishspeech.py` | L1-37 |
| `main/xiaozhi-server/core/providers/asr/base.py` | L1-19 |
| `main/xiaozhi-server/core/providers/tts/gpt_sovits_v3.py` | L1-30 |
| `main/xiaozhi-server/core/providers/tts/base.py` | L44-88 |
| `main/xiaozhi-server/core/handle/receiveAudioHandler.py` | L1-48 |
| `main/xiaozhi-server/core/providers/asr/tencent.py` | L55-78 |
| `main/xiaozhi-server/core/providers/tts/siliconflow.py` | L1-39 |
| `main/xiaozhi-server/core/providers/tts/edge.py` | L1-27 |
| `main/xiaozhi-server/plugins_func/functions/play_music.py` | L158-196 |
| `main/xiaozhi-server/core/providers/tts/doubao.py` | L1-24 |
| `main/xiaozhi-server/core/utils/p3.py` | L1-33 |
| `main/xiaozhi-server/core/utils/asr.py` | L1-24 |

</details>

---



---

好的，我们来分析一下这个问题。

根据搜索结果，项目中使用 FunASR 的代码位于 `main/xiaozhi-server/core/providers/asr/fun_local.py`。在这个文件中，`speech_to_text` 方法负责将语音转换为文本，并在 `finally` 块中根据 `self.delete_audio_file` 的值决定是否删除处理过程中生成的音频文件。

```python
# main/xiaozhi-server/core/providers/asr/fun_local.py
# ... (部分代码)
class ASRProvider(ASRProviderBase):
    def __init__(self, config: dict, delete_audio_file: bool):
        self.model_dir = config.get("model_dir")
        self.output_dir = config.get("output_dir")  # 修正配置键名
        self.delete_audio_file = delete_audio_file # 这个参数决定是否删除文件
        # ... (部分代码)

    async def speech_to_text(self, opus_data: List[bytes], session_id: str) -> Tuple[Optional[str], Optional[str]]:
        """语音转文本主处理逻辑"""
        file_path = None
        try:
            # 保存音频文件
            start_time = time.time()
            file_path = self.save_audio_to_file(opus_data, session_id)
            logger.bind(tag=TAG).debug(f"音频文件保存耗时: {time.time() - start_time:.3f}s | 路径: {file_path}")

            # 语音识别
            start_time = time.time()
            # 注意这里的调用方式和参数与你提供的参考代码不同
            result = self.model.generate(
                input=file_path,
                cache={},
                language="auto",
                use_itn=True,
                batch_size_s=60,
            )
            text = rich_transcription_postprocess(result[0]["text"])
            logger.bind(tag=TAG).debug(f"语音识别耗时: {time.time() - start_time:.3f}s | 结果: {text}")

            return text, file_path

        except Exception as e:
            logger.bind(tag=TAG).error(f"语音识别失败: {e}", exc_info=True)
            return "", None

        finally:
            # 文件清理逻辑
            if self.delete_audio_file and file_path and os.path.exists(file_path): # 这里会删除文件
                try:
                    os.remove(file_path)
                    logger.bind(tag=TAG).debug(f"已删除临时音频文件: {file_path}")
                except Exception as e:
                     logger.bind(tag=TAG).error(f"文件删除失败: {file_path} | 错误: {e}")

```

**如何保留中间音频文件：**

为了方便你调试，我会修改 `fun_local.py` 文件，注释掉删除文件的代码行，这样每次识别后都会保留生成的 `.wav` 文件在 `tmp/` 目录下（根据 `config.yaml` 配置）。

**为什么识别结果不同：**

仔细对比项目中的 `self.model.generate` 调用和你的参考代码，可以发现一个关键区别：

*   **项目代码 (`fun_local.py`)**:
    *   直接将完整的 `file_path` 作为 `input`。
    *   使用了 `language="auto"`, `use_itn=True`, `batch_size_s=60` 等参数。
    *   **没有使用流式处理** 相关的参数，如 `is_final`, `chunk_size`, `encoder_chunk_look_back`, `decoder_chunk_look_back`。
*   **参考代码**:
    *   将音频分割成 `speech_chunk` 进行**流式处理**。
    *   使用了 `is_final`, `chunk_size`, `encoder_chunk_look_back`, `decoder_chunk_look_back` 来控制流式识别的行为。

**结论：**

项目中 FunASR 的调用方式是**非流式**的，它将整个音频文件一次性传递给模型进行识别。而你的参考代码是**流式**的，模拟了实时语音输入的情况。这两种不同的调用方式（尤其是流式处理涉及的上下文信息和缓存机制）很可能是导致识别结果差异的主要原因。

我现在将修改 `fun_local.py` 以保留音频文件。你可以检查这些文件，并考虑修改项目中的 FunASR 调用方式，使其与你的参考代码（流式调用）保持一致，看看是否能解决识别错误的问题。



---

```diff
          finally:
              # 文件清理逻辑
-             if self.delete_audio_file and file_path and os.path.exists(file_path):
-                 try:
-                     os.remove(file_path)
-                     logger.bind(tag=TAG).debug(f"已删除临时音频文件: {file_path}")
-                 except Exception as e:
-                     logger.bind(tag=TAG).error(f"文件删除失败: {file_path} | 错误: {e}")
+             # if self.delete_audio_file and file_path and os.path.exists(file_path): # 注释掉这行以保留文件
+             #     try:
+             #         os.remove(file_path)
+             #         logger.bind(tag=TAG).debug(f"已删除临时音频文件: {file_path}")
+             #     except Exception as e:
+             #          logger.bind(tag=TAG).error(f"文件删除失败: {file_path} | 错误: {e}")
+             pass # 添加 pass 避免语法错误
  
```

---



---

我已经修改了 `main/xiaozhi-server/core/providers/asr/fun_local.py` 文件，注释掉了删除临时音频文件的代码。现在，每次进行语音识别后，生成的 `.wav` 文件会保留在 `main/xiaozhi-server/tmp/` 目录下。

你可以检查这些保存下来的音频文件，确认它们是否符合预期。

如前所述，项目中的 FunASR 调用方式（非流式）与你提供的参考代码（流式）存在显著差异，这很可能是导致识别结果不同的根本原因。如果你希望项目中使用流式识别，你需要修改 `fun_local.py` 中的 `speech_to_text` 方法，使其按照流式方式处理音频并调用 `model.generate`，传入类似 `chunk_size`, `is_final` 等参数。

---

_**User**_

生成的中间asr语音没有问题，应该是生成的text相关函数配置不正确，请参考模型调用的链接说明。@https://github.com/modelscope/FunASR?tab=readme-ov-file#installation 请保留现有项目代码，函数保留为_backup，然后按照官方文档重现一个调用生成的函数用于测试。

---

_**Assistant**_



---

OK，我理解你的需求了。你想保留当前非流式的 `speech_to_text` 实现作为备份，并根据 FunASR 官方文档中的流式调用示例，创建一个新的 `speech_to_text` 函数，以便对比和测试流式识别的效果。

我将执行以下操作：

1.  在 `main/xiaozhi-server/core/providers/asr/fun_local.py` 文件中，将现有的 `speech_to_text` 函数重命名为 `speech_to_text_backup`。
2.  添加一个新的 `speech_to_text` 函数，该函数将：
    *   仍然调用 `save_audio_to_file` 来保存完整的 `.wav` 文件（因为你说中间文件没问题，方便继续保留它进行调试）。
    *   将输入的 Opus 数据解码为 PCM 字节流。
    *   根据 FunASR 流式示例，定义 `chunk_size` 等参数。
    *   将 PCM 数据分块。
    *   循环调用 `self.model.generate` 处理每个块，并传入流式参数（`chunk_size`, `cache`, `is_final`, `encoder_chunk_look_back`, `decoder_chunk_look_back`）。
    *   聚合最终的识别结果。
    *   返回识别文本和保存的文件路径。

请注意，这个新的实现假设你当前加载的模型（在 `config.yaml` 中配置，似乎是 `models/SenseVoiceSmall`）支持流式接口。如果该模型不支持，调用可能会失败。官方示例使用的是 `paraformer-zh-streaming`。如果新函数报错，可能需要检查模型兼容性。

我现在开始修改代码。


---

_**User**_

不使用官方的paraformer-zh-streaming模型，好像官方的也不是流式调用的，就用基本的方案即可。from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess

model_dir = "iic/SenseVoiceSmall"

model = AutoModel(
    model=model_dir,
    vad_model="fsmn-vad",
    vad_kwargs={"max_single_segment_time": 30000},
    device="cuda:0",
)

# en
res = model.generate(
    input=f"{model.model_path}/example/en.mp3",
    cache={},
    language="auto",  # "zn", "en", "yue", "ja", "ko", "nospeech"
    use_itn=True,
    batch_size_s=60,
    merge_vad=True,  #
    merge_length_s=15,
)
text = rich_transcription_postprocess(res[0]["text"])
print(text)

帮我修改源文件代码

---

_**Assistant**_



---

好的，明白了。你希望我直接修改现有的 `speech_to_text` 函数，不再创建备份，并确保其调用 `model.generate` 的方式与你提供的最新示例代码一致，特别是加入 `merge_vad=True` 和 `merge_length_s=15` 这两个参数。

我将直接修改 `main/xiaozhi-server/core/providers/asr/fun_local.py` 文件中的 `speech_to_text` 函数，在 `self.model.generate` 调用中添加这两个参数。



---

```diff
                  use_itn=True,
                  batch_size_s=60,
+                 merge_vad=True,
+                 merge_length_s=15,
              )
              text = rich_transcription_postprocess(result[0]["text"])
```

---



---

我已经修改了 `main/xiaozhi-server/core/providers/asr/fun_local.py` 文件。在 `speech_to_text` 函数中调用 `self.model.generate` 时，已经添加了 `merge_vad=True` 和 `merge_length_s=15` 这两个参数。

你可以重新运行你的项目，看看这次的识别结果是否符合预期。

---

_**User**_

我找到问题了，print(self.model_dir, type(self.model_dir)) 输出结果为models/SenseVoiceSmall, 模型没有被正确引用，正确的模型路径应该是：../../../models/SenseVoiceSmall 请帮我调整相关的代码。或者是配置文件。

---

_**Assistant**_



---

好的，明白了。问题出在 `config.yaml` 中为 FunASR 配置的 `model_dir` 路径 `models/SenseVoiceSmall`。这个路径通常是相对于项目运行的根目录来解析的，而不是相对于 `fun_local.py` 文件本身。

根据你提供的正确相对路径 `../../../models/SenseVoiceSmall`（相对于 `main/xiaozhi-server/core/providers/asr/fun_local.py`），我们可以推断出 `models` 文件夹位于 `xiaozhi-esp32-server` 这个工作区的根目录下。

如果你的 Python 应用是从 `main/xiaozhi-server/` 这个目录启动的，那么在 `config.yaml` 中，正确的相对路径应该是 `../models/SenseVoiceSmall`。

我将修改 `config.yaml` 文件，将 FunASR 的 `model_dir` 更新为正确的相对路径。同时，我也会移除你在 `fun_local.py` 中添加的调试打印语句。

1.  **读取 `config.yaml` 中 FunASR 的配置部分。**
2.  **修改 `config.yaml` 中 FunASR 的 `model_dir`。**
3.  **移除 `fun_local.py` 中的 `print` 语句。**



---

```diff
      def __init__(self, config: dict, delete_audio_file: bool):
          self.model_dir = config.get("model_dir")
-         print(self.model_dir, type(self.model_dir))
          self.output_dir = config.get("output_dir")  # 修正配置键名
          self.delete_audio_file = delete_audio_file
```

---

Read file: main/xiaozhi-server/config.yaml

---

```diff
    FunASR:
      type: fun_local
-     model_dir: models/SenseVoiceSmall
+     model_dir: ../models/SenseVoiceSmall
      output_dir: tmp/
    SherpaASR:
```

---

我已经修改了 `main/xiaozhi-server/config.yaml` 文件，将 FunASR 的 `model_dir` 更新为 `../models/SenseVoiceSmall`。同时，之前也移除了 `fun_local.py` 中的调试打印语句。

请你重新运行项目，确认 FunASR 模型现在是否能被正确加载和调用。

---

_**User**_

只添加../上级目录就可以了吗？我在测试用例的时候：
    # 读取音频文件
    audio_file_path = r"C:\Users\51402\Desktop\test.wav"

    from funasr import AutoModel
    from funasr.utils.postprocess_utils import rich_transcription_postprocess

    # model_dir = "iic/SenseVoiceSmall"
    # model_dir = (
    #     r"D:\Project\xiaozhi-esp32-server-1\main\xiaozhi-server\models\SenseVoiceSmall"
    # )
    model_dir = "../../../models/SenseVoiceSmall"

    model = AutoModel(
        model=model_dir,
        # vad_model="fsmn-vad",
        vad_kwargs={"max_single_segment_time": 30000},
        device="cuda:0",
    )

    # en
    res = model.generate(
        input=audio_file_path,
        cache={},
        language="auto",  # "zn", "en", "yue", "ja", "ko", "nospeech"
        use_itn=True,
        batch_size_s=60,
        merge_vad=True,  #
        merge_length_s=15,
    )
    text = rich_transcription_postprocess(res[0]["text"])
    print(text)

如果仅仅是引用上级，是找不到模型路径的。你在分析一下路径调用是否正确。

---

_**Assistant**_



---

你说得对，非常抱歉之前的分析不够全面。

仅仅修改 `config.yaml` 中的相对路径 `../models/SenseVoiceSmall` 确实可能在不同的执行环境下导致问题，因为它依赖于**运行脚本时的当前工作目录 (CWD)**。

你的测试脚本直接运行时，它的 CWD 可能与主应用程序启动时的 CWD 不同，因此同样的相对路径会解析到不同的位置。路径 `../../../models/SenseVoiceSmall` 在你的测试脚本中有效，是因为它相对于该脚本文件的位置是正确的。

**更健壮的解决方案：**

我们应该在代码中动态计算模型的绝对路径，而不是依赖于 `config.yaml` 中相对于 CWD 的路径。这样可以确保无论从哪里启动应用，都能找到模型文件。

**修改步骤：**

1.  **修改 `config.yaml`:** 将 FunASR 的 `model_dir` 改回相对于项目根目录的路径，即 `models/SenseVoiceSmall`。这是更标准的做法，表示模型目录在项目根目录下的 `models` 文件夹内。
2.  **修改 `main/xiaozhi-server/core/providers/asr/fun_local.py`:**
    *   在 `__init__` 方法中，获取当前文件 (`fun_local.py`) 的绝对路径。
    *   根据当前文件路径推算出项目的根目录路径。
    *   读取 `config.yaml` 中的相对模型路径 (`models/SenseVoiceSmall`)。
    *   将项目根目录路径和相对模型路径拼接起来，得到模型的绝对路径。
    *   使用这个绝对路径来初始化 `AutoModel`。

这样修改后，模型路径的查找将不再依赖于启动脚本时的 CWD。

**第一步：修改 `config.yaml`**
我将把 `model_dir` 改回 `models/SenseVoiceSmall`。



---

```diff
    FunASR:
      type: fun_local
-     model_dir: ../models/SenseVoiceSmall
+     model_dir: models/SenseVoiceSmall
      output_dir: tmp/
    SherpaASR:
```

---

**第二步：修改 `fun_local.py` 以计算绝对路径**
我将修改 `__init__` 方法来动态计算模型的绝对路径。



---

```diff
  class ASRProvider(ASRProviderBase):
      def __init__(self, config: dict, delete_audio_file: bool):
-         self.model_dir = config.get("model_dir")
-         self.output_dir = config.get("output_dir")  # 修正配置键名
+         model_dir_relative = config.get("model_dir")
+         self.output_dir = config.get("output_dir")
          self.delete_audio_file = delete_audio_file
+ 
+         # 获取当前文件的绝对路径
+         current_file_path = os.path.abspath(__file__)
+         # 推算项目根目录 (假设 fun_local.py 在 main/xiaozhi-server/core/providers/asr/ 下)
+         project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_file_path)))))
+         # 计算模型的绝对路径
+         self.model_dir_absolute = os.path.join(project_root, 'main', 'xiaozhi-server', model_dir_relative)
+ 
+         logger.bind(tag=TAG).info(f"Calculated absolute model path: {self.model_dir_absolute}") # 添加日志方便调试
  
          # 确保输出目录存在
          os.makedirs(self.output_dir, exist_ok=True)
          with CaptureOutput():
              self.model = AutoModel(
-                 model=self.model_dir,
+                 model=self.model_dir_absolute, # 使用绝对路径
                  vad_kwargs={"max_single_segment_time": 30000},
                  disable_update=True,
```

---



---



---

好的，我们采用更健壮的方法来处理路径问题。

1.  **`config.yaml`**: `model_dir` 保持为 `models/SenseVoiceSmall`，这表示相对于 `main/xiaozhi-server` 这个应用根目录的路径。 (这一步已完成)
2.  **`fun_local.py`**: 修改 `__init__` 方法，通过 `__file__` 获取当前脚本位置，推算出应用根目录 (`main/xiaozhi-server`) 的绝对路径，然后结合 `config.yaml` 中的相对路径 (`models/SenseVoiceSmall` 和 `tmp/`) 来计算出模型目录和输出目录的绝对路径。

这样可以确保无论从哪里启动应用，都能正确定位模型和输出目录。

我将应用这个修改到 `main/xiaozhi-server/core/providers/asr/fun_local.py`。



---

_**User**_

报错了

---

_**Assistant**_



---

确实报错了，仔细看日志：

```
Calculated absolute model path: D:\Project\xiaozhi-esp32-server\main\main\xiaozhi-server\models/SenseVoiceSmall
```

这个计算出的绝对路径 `D:\Project\xiaozhi-esp32-server\main\main\xiaozhi-server\models/SenseVoiceSmall` 是错误的，里面包含了重复的 `main` 目录。

这是因为我之前推算项目根目录的逻辑和拼接逻辑组合起来后产生了错误。

**修正方案：**

我们将调整 `fun_local.py` 中的路径计算逻辑：

1.  计算出 `main/xiaozhi-server` 这个**应用根目录**的绝对路径。它应该是 `fun_local.py` 往上 4 级目录。
2.  然后使用这个应用根目录路径，结合 `config.yaml` 中的相对路径 (`models/SenseVoiceSmall` 和 `tmp/`) 来生成模型目录和输出目录的绝对路径。

我将修改 `main/xiaozhi-server/core/providers/asr/fun_local.py` 中的 `__init__` 方法来修正路径计算。



---

```diff
      def __init__(self, config: dict, delete_audio_file: bool):
          model_dir_relative = config.get("model_dir")
-         self.output_dir = config.get("output_dir")
+         output_dir_relative = config.get("output_dir")
          self.delete_audio_file = delete_audio_file
  
          # 获取当前文件的绝对路径
          current_file_path = os.path.abspath(__file__)
-         # 推算项目根目录 (假设 fun_local.py 在 main/xiaozhi-server/core/providers/asr/ 下)
-         project_root = os.path.dirname(
-             os.path.dirname(
-                 os.path.dirname(os.path.dirname(os.path.dirname(current_file_path)))
-             )
-         )
-         # 计算模型的绝对路径
-         self.model_dir_absolute = os.path.join(
-             project_root, "main", "xiaozhi-server", model_dir_relative
-         )
+         # 推算应用根目录 (main/xiaozhi-server)
+         app_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_file_path))))
+ 
+         # 计算模型和输出目录的绝对路径
+         self.model_dir_absolute = os.path.join(app_root, model_dir_relative)
+         self.output_dir_absolute = os.path.join(app_root, output_dir_relative)
  
-         logger.bind(tag=TAG).info(
-             f"Calculated absolute model path: {self.model_dir_absolute}"
-         )  # 添加日志方便调试
+         logger.bind(tag=TAG).info(f"App root: {app_root}")
+         logger.bind(tag=TAG).info(f"Calculated absolute model path: {self.model_dir_absolute}")
+         logger.bind(tag=TAG).info(f"Calculated absolute output path: {self.output_dir_absolute}")
  
          # 确保输出目录存在
-         os.makedirs(self.output_dir, exist_ok=True)
+         os.makedirs(self.output_dir_absolute, exist_ok=True)
          with CaptureOutput():
              self.model = AutoModel(
          """将Opus音频数据解码并保存为WAV文件"""
          file_name = f"asr_{session_id}_{uuid.uuid4()}.wav"
-         file_path = os.path.join(self.output_dir, file_name)
+         # 使用绝对路径保存文件
+         file_path = os.path.join(self.output_dir_absolute, file_name)
  
          decoder = opuslib_next.Decoder(16000, 1)  # 16kHz, 单声道
```

---



---

我已经更新了 `main/xiaozhi-server/core/providers/asr/fun_local.py` 中的路径计算逻辑。

这次的修改：

1.  正确地推算出应用根目录 (`main/xiaozhi-server`) 的绝对路径。
2.  使用该应用根目录路径结合 `config.yaml` 中的相对路径 (`models/SenseVoiceSmall` 和 `tmp/`) 来生成模型目录 (`self.model_dir_absolute`) 和输出目录 (`self.output_dir_absolute`) 的绝对路径。
3.  确保 `AutoModel` 初始化和 `save_audio_to_file` 函数都使用计算出的绝对路径。
4.  添加了更详细的日志记录，方便再次检查路径是否正确。

请再次运行你的项目，查看日志中打印出的 `App root`, `Calculated absolute model path`, 和 `Calculated absolute output path` 是否符合预期，以及之前的错误是否已解决。

---

_**User**_

不能使用绝对路径，只有以下的相对路径的调用形式才是正确的。
    model_dir = "../../../models/SenseVoiceSmall"

    model = AutoModel(
        model=model_dir,
        # vad_model="fsmn-vad",
        vad_kwargs={"max_single_segment_time": 30000},
        device="cuda:0",
    )

---

_**Assistant**_



---

好的，我理解了。尽管通常推荐使用绝对路径来避免环境依赖，但既然你确认在你的测试中只有 `../../../models/SenseVoiceSmall` 这个相对于 `fun_local.py` 的特定相对路径能够正确加载模型，那我们就按照这个方式来修改。

**修改方案：**

1.  在 `main/xiaozhi-server/core/providers/asr/fun_local.py` 的 `__init__` 方法中：
    *   我们将直接在 `AutoModel` 初始化时使用硬编码的相对路径 `../../../models/SenseVoiceSmall` 作为 `model` 参数的值。
    *   对于输出目录 `output_dir`，为了确保临时文件仍然保存在项目预期的 `main/xiaozhi-server/tmp/` 目录下，我们将继续使用之前计算出的绝对路径 `self.output_dir_absolute`。这可以避免临时文件被意外地写到项目结构之外。
    *   移除不再需要的 `model_dir_relative` 读取和相关日志。

我将应用这些修改。



---

```diff
  class ASRProvider(ASRProviderBase):
      def __init__(self, config: dict, delete_audio_file: bool):
-         model_dir_relative = config.get("model_dir")
+         # model_dir_relative = config.get("model_dir") # No longer needed
          output_dir_relative = config.get("output_dir")
          self.delete_audio_file = delete_audio_file
          )
  
-         # 计算模型和输出目录的绝对路径
-         self.model_dir_absolute = os.path.join(app_root, model_dir_relative)
+         # --- 模型路径修改 --- 
+         # 根据用户要求，直接使用相对于此文件的固定相对路径
+         model_path_for_init = "../../../models/SenseVoiceSmall" 
+ 
+         # --- 输出目录保持绝对路径计算 --- 
          self.output_dir_absolute = os.path.join(app_root, output_dir_relative)
  
          logger.bind(tag=TAG).info(f"App root: {app_root}")
+         # logger.bind(tag=TAG).info(f"Calculated absolute model path: {self.model_dir_absolute}") # Removed
          logger.bind(tag=TAG).info(
-             f"Calculated absolute model path: {self.model_dir_absolute}"
+             f"Using relative model path for init: {model_path_for_init}"
          )
          logger.bind(tag=TAG).info(
          with CaptureOutput():
              self.model = AutoModel(
-                 model=self.model_dir_absolute,  # 使用绝对路径
+                 model=model_path_for_init,  # 使用用户指定的相对路径
                  vad_kwargs={"max_single_segment_time": 30000},
                  disable_update=True,
```

---



---

好的，我已经按照你的要求修改了 `main/xiaozhi-server/core/providers/asr/fun_local.py`。

具体的改动如下：

1.  在 `__init__` 方法中，初始化 `AutoModel` 时，`model` 参数现在直接使用了你指定的相对路径字符串 `"../../../models/SenseVoiceSmall"`。
2.  计算和使用模型绝对路径 `self.model_dir_absolute` 的代码已被移除。
3.  输出目录 `self.output_dir_absolute` 的计算逻辑保持不变（基于应用根目录和 `config.yaml`），并且 `save_audio_to_file` 函数仍然使用这个绝对路径来保存临时 `.wav` 文件。
4.  相关的日志信息也已更新。

请再次运行你的项目，确认这次是否能够正确加载模型并正常运行。

---

_**User**_

我刚才可能表达的不准确。
model_dir = "D:\\Project\\xiaozhi-esp32-server-1\\main\\xiaozhi-server\\models\\SenseVoiceSmall"
以下这种写法，也可以被模型识别。

---

_**Assistant**_



---

好的，谢谢你的澄清！

你确认了像 `D:\Project\xiaozhi-esp32-server-1\main\xiaozhi-server\models\SenseVoiceSmall` 这样的**绝对路径**也可以被 `AutoModel` 正确识别。

这实际上加强了我们之前的想法：**在代码中动态计算模型的绝对路径是最稳妥的方案**。

原因如下：

1.  **硬编码绝对路径** (如 `D:\...`) 的问题：如果项目移动到其他目录或部署到其他机器，这个路径就会失效。
2.  **硬编码相对路径** (如 `../../../models/SenseVoiceSmall`) 的问题：它依赖于运行 Python 脚本时的“当前工作目录”（Current Working Directory, CWD）。如果你的主程序和测试脚本从不同的目录启动，这个相对路径的解析结果就会不同，导致在一个地方能用，在另一个地方不能用。
3.  **动态计算绝对路径**：通过代码（利用 `__file__` 和 `os.path.dirname`）计算出模型目录相对于当前代码文件的准确位置，得到一个在任何环境下都有效的绝对路径。这是最不容易出错、最推荐的方式。

我们之前第二次尝试动态计算绝对路径的代码逻辑（也就是我在上上次回复中修改后的版本）计算出的路径应该是：
`D:\Project\xiaozhi-esp32-server\main\xiaozhi-server\models\SenseVoiceSmall`
（假设你的项目根目录是 `D:\Project\xiaozhi-esp32-server\`）

既然你确认了这个绝对路径是有效的，那么我们应该恢复到那个动态计算绝对路径的版本。这样既能保证模型被正确加载，又能避免硬编码带来的问题。

我将把 `main/xiaozhi-server/core/providers/asr/fun_local.py` 的 `__init__` 方法恢复到我们第二次尝试动态计算绝对路径时的状态。



---



---

